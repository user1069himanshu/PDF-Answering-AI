{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4sSRzmEMDz4"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW, pipeline\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "# Step 1: PDF Text Extraction\n",
        "def get_text_from_pdf(file_path):\n",
        "    document = fitz.open(file_path)\n",
        "    extracted_text = \"\"\n",
        "    for page in document:\n",
        "        extracted_text += page.get_text()\n",
        "    return extracted_text\n",
        "\n",
        "# Step 2: Load the SQuAD dataset\n",
        "dataset = load_dataset('squad')\n",
        "\n",
        "# Select a smaller subset of the dataset for quicker training\n",
        "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "eval_subset = dataset[\"validation\"].shuffle(seed=42).select(range(200))\n",
        "\n",
        "# Step 3: Load a pre-trained transformer model\n",
        "model_identifier = \"distilbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_identifier)\n",
        "\n",
        "# Step 4: Tokenize the dataset\n",
        "def preprocess_data(samples):\n",
        "    questions = [q.strip() for q in samples[\"question\"]]\n",
        "    encodings = tokenizer(\n",
        "        questions,\n",
        "        samples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mappings = encodings.pop(\"offset_mapping\")\n",
        "    sample_mappings = encodings.pop(\"overflow_to_sample_mapping\")\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mappings):\n",
        "        input_ids = encodings[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        sequence_ids = encodings.sequence_ids(i)\n",
        "        sample_index = sample_mappings[i]\n",
        "        answers = samples[\"answers\"][sample_index]\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_idx = 0\n",
        "            while sequence_ids[token_start_idx] != 1:\n",
        "                token_start_idx += 1\n",
        "\n",
        "            token_end_idx = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_idx] != 1:\n",
        "                token_end_idx -= 1\n",
        "\n",
        "            if not (offsets[token_start_idx][0] <= start_char and offsets[token_end_idx][1] >= end_char):\n",
        "                start_positions.append(cls_index)\n",
        "                end_positions.append(cls_index)\n",
        "            else:\n",
        "                while token_start_idx < len(offsets) and offsets[token_start_idx][0] <= start_char:\n",
        "                    token_start_idx += 1\n",
        "                start_positions.append(token_start_idx - 1)\n",
        "\n",
        "                while offsets[token_end_idx][1] >= end_char:\n",
        "                    token_end_idx -= 1\n",
        "                end_positions.append(token_end_idx + 1)\n",
        "\n",
        "    encodings[\"start_positions\"] = start_positions\n",
        "    encodings[\"end_positions\"] = end_positions\n",
        "    return encodings\n",
        "\n",
        "tokenized_train_data = train_subset.map(preprocess_data, batched=True, remove_columns=train_subset.column_names)\n",
        "tokenized_eval_data = eval_subset.map(preprocess_data, batched=True, remove_columns=eval_subset.column_names)\n",
        "\n",
        "# Step 5: Create DataLoaders\n",
        "train_loader = DataLoader(tokenized_train_data, shuffle=True, batch_size=16, collate_fn=default_data_collator)\n",
        "eval_loader = DataLoader(tokenized_eval_data, batch_size=16, collate_fn=default_data_collator)\n",
        "\n",
        "# Step 6: Set Up Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "num_epochs = 1  # Reduce the number of epochs for quicker training\n",
        "total_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=total_training_steps // 3, gamma=0.1)\n",
        "\n",
        "# Step 7: Training Loop\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "progress_bar = tqdm(range(total_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./my-fine-tuned-model\")\n",
        "tokenizer.save_pretrained(\"./my-fine-tuned-model\")\n",
        "\n",
        "# Step 8: Evaluate the Model\n",
        "qa_pipeline = pipeline('question-answering', model=\"./my-fine-tuned-model\", tokenizer=\"./my-fine-tuned-model\")\n",
        "\n",
        "# Example usage\n",
        "pdf_file_path = 'example.pdf'  # Replace with the path to your PDF file\n",
        "pdf_text = get_text_from_pdf(pdf_file_path)\n",
        "user_question = \"Who won Super Bowl 50?\"\n",
        "answer = qa_pipeline(question=user_question, context=pdf_text)\n",
        "print(f\"Answer: {answer['answer']}\")\n"
      ]
    }
  ]
}